{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd029ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aadb5f6",
   "metadata": {},
   "source": [
    "# Phase 1 : AI LIBRARIAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef40ea",
   "metadata": {},
   "source": [
    " Goal: a function that returns a dictionary where keys are section titles and values are the text of those sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be990397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymupdf\n",
    "import fitz\n",
    "# function\n",
    "def parsing_paper(pdf_path):\n",
    "    docs = fitz.open(pdf_path)\n",
    "    # Empty list/Collection basket\n",
    "    pages_data = []\n",
    "\n",
    "    for pages in docs:\n",
    "        texts = pages.get_text() # extract text from current page\n",
    "        pages_data.append(texts)\n",
    "    full_text = \"\\n\".join(pages_data)\n",
    "\n",
    "    return full_text    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9bd503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vulnerable Agent Identification in Large-Scale\n",
      "Multi-Agent Reinforcement Learning\n",
      "Simin Li\n",
      "SKLSDE Lab\n",
      "Beihang University\n",
      "lisiminsimon@buaa.edu.cn\n",
      "Zheng Yuwei\n",
      "School of Computer Science and Engineering\n",
      "Beihang University\n",
      "neozhengyuwei@gmail.com\n",
      "Zihao Mao\n",
      "Institute of Artificial Intelligence\n",
      "Beihang University\n",
      "zihaomao@buaa.edu.cn\n",
      "Linhao Wang\n",
      "School of Computer Science and Engineering\n",
      "Beihang University\n",
      "linhao.wang@buaa.edu.cn\n",
      "Ruixiao Xu\n",
      "School of Computer Science and Engineering\n",
      "Beihang University\n",
      "xuruixiao@buaa.edu.cn\n",
      "Chengdong Ma\n",
      "Institute of Artificial Intelligence\n",
      "Peking University\n",
      "machengdong@stu.xmu.edu.cn\n",
      "Xin Yu\n",
      "Institute of automation\n",
      "Chinese academy of science\n",
      "nlsdeyuxin@buaa.edu.cn\n",
      "Yuqing Ma\n",
      "School of Computer Science and Engineering\n",
      "Beihang Universit\n",
      "yuqingma@buaa.edu.cn\n",
      "Qi Dou\n",
      "Department of Computer Science and Engineering\n",
      "The Chinese University of Hong Kong qidou@cuhk.edu.hk\n",
      "Xin Wang\n",
      "Institute of Artificial Intelligence\n",
      "Beihang University\n",
      "wangxin_1993@buaa.edu.cn\n",
      "Jie Luo\n",
      "School of Computer Science and Engineering\n",
      "Beihang University\n",
      "luojie@buaa.edu.cn\n",
      "Bo An\n",
      "College of Computing and Data Science\n",
      "Nanyang Technological University\n",
      "boan@ntu.edu.sg\n",
      "Yaodong Yang\n",
      "Institute of Artificial Intelligence & BigAI\n",
      "Peking University\n",
      "yaodong.yang@pku.edu.cn\n",
      "Weifeng Lv\n",
      "School of Computer Science and Engineering\n",
      "Beihang University\n",
      "lwf@buaa.edu.cn\n",
      "Xianglong Liu∗\n",
      "School of Computer Science and Engineering\n",
      "Beihang University\n",
      "xlliu@buaa.edu.cn\n",
      "Abstract\n",
      "∗Corresponding Author\n",
      "arXiv:2509.15103v1  [cs.MA]  18 Sep 2025\n",
      "\n",
      "Partial agent failure becomes inevitable when systems scale up, making it crucial to\n",
      "identify the subset of agents whose compromise would most severely degrade over-\n",
      "all performance. In this paper, we study this Vulnerable Agent Identification (VAI)\n",
      "problem in large-scale multi-agent reinforcement learning (MARL). We frame\n",
      "VAI as a Hierarchical Adversarial Decentralized Mean Field Control (HAD-MFC),\n",
      "where the upper level involves an NP-hard combinatorial task of selecting the most\n",
      "vulnerable agents, and the lower level learns worst-case adversarial policies for\n",
      "these agents using mean-field MARL. The two problems are coupled together,\n",
      "making HAD-MFC difficult to solve. To solve this, we first decouple the hierarchi-\n",
      "cal process by Fenchel-Rockafellar transform, resulting a regularized mean-field\n",
      "Bellman operator for upper level that enables independent learning at each level,\n",
      "thus reducing computational complexity. We then reformulate the upper-level com-\n",
      "binatorial problem as a MDP with dense rewards from our regularized mean-field\n",
      "Bellman operator, enabling us to sequentially identify the most vulnerable agents\n",
      "by greedy and RL algorithms. This decomposition provably preserves the optimal\n",
      "solution of the original HAD-MFC. Experiments show our method effectively\n",
      "identifies more vulnerable agents in large-scale MARL and the rule-based system,\n",
      "fooling system into worse failures, and learns a value function that reveals the\n",
      "vulnerability of each agent.\n",
      "1\n",
      "Introduction\n",
      "Mean-field multi-agent reinforcement learning (MARL) [1, 2, 3, 4] has significantly enhanced the\n",
      "scalability of MARL through mean-field approximation, making it applicable to many large-scale\n",
      "real-world applications, such as robot swarm control [5, 6], voltage control [7], and traffic control\n",
      "[8]. However, given the large number of agents in such systems, it is likely that a small portion will\n",
      "deviate from the original policy during real-world deployment. For instance, in a thousand-robot\n",
      "swarm, individual robots may encounter action uncertainty [9] from software or hardware errors\n",
      "[10], environmental hazards [11], or even be controlled by adversaries [12, 13, 14, 15, 16]. As agent\n",
      "policies are interconnected in mean-field MARL, it is crucial to evaluate the impact of the failure of a\n",
      "small group of agents on the entire system.\n",
      "In this paper, we focus on vulnerable agent identification (VAI) in large-scale MARL systems. This\n",
      "allows practitioners to enhance system robustness by providing these agents with targeted monitoring\n",
      "and protection. We further evaluate the system’s worst-case robustness under adversarial attacks [14],\n",
      "offering practitioners the worst-case performance of the system.\n",
      "Critics may argue that vulnerable agents do not exist, as theoretical Mean-Field Controls assume all\n",
      "agents take identical actions [17, 3]. However, in real-world large-scale MARL systems, agents often\n",
      "have different initializations, local states, or interact with limited neighbors [6, 1], leading to agent\n",
      "variability. In such cases, a mean-field approximation remains relevant but does not assume full agent\n",
      "homogeneity. Research in network science has tackled influence maximization [18, 19, 20], which\n",
      "seeks to select a group of nodes in rule-based social networks to maximize their influence. However,\n",
      "these studies typically assume known graph structures, transition dynamics, and influence rules,\n",
      "which are absent in our setting. Identifying vulnerable agents has also been explored in small-scale\n",
      "MARL systems [21, 22, 23]. The primary challenge arises from scale: a 10-agent system has only\n",
      "\u000010\n",
      "1\n",
      "\u0001\n",
      "possible scenarios, while a 1000-agent system yields\n",
      "\u00001000\n",
      "100\n",
      "\u0001\n",
      "scenarios, an increase by a factor of\n",
      "10139. This represents a coupled problem where the upper level is a combinatorial problem, and the\n",
      "lower level involves mean-field MARL, making the complexity the central difficulty.\n",
      "We begin by analyzing the complexity of the problem, which we formulate as a Hierarchical Adver-\n",
      "sarial Decentralized Mean Field Control (HAD-MFC). At the upper level, the task is to select M\n",
      "most vulnerable agents from a total of N, resulting in a combinatorial problem with complexity\n",
      "\u0000N\n",
      "M\n",
      "\u0001\n",
      ".\n",
      "We show that this problem is NP-hard by reducing it to the generalized maximum coverage problem\n",
      "[24]. The lower level involves a mean-field MARL task, where an adversarial policy [14] is trained\n",
      "for the selected M vulnerable agents to assess the system’s worst-case robustness. Consequently,\n",
      "the overall challenge requires solving an NP-hard upper-level problem followed by a downstream\n",
      "mean-field MARL task.\n",
      "2\n",
      "\n",
      "To address the coupled problem of vulnerable agent identification, we first decouple the upper-level\n",
      "attacker with the lower-level one. Given a selected set of vulnerable agents at the upper level,\n",
      "we propose regularized mean-field Bellman operator that can evaluate the value function directly,\n",
      "under worst-case attack at lower level. The operator is derived by applying the Fenchel-Rockafellar\n",
      "transform [25, 26] to the uncertainty of vulnerable agents at the lower level. To solve the NP-hard\n",
      "upper-level problem, we formulate it as a MDP with dense rewards by the value function learned from\n",
      "our regularized mean-field Bellman operator, and solve it by greedy and RL algorithms. Next, we\n",
      "train a MARL policy on these selected vulnerable agents to evaluate the robustness of the large-scale\n",
      "MARL system. Finally, we prove that decomposing the original HAD-MFC into upper- and lower-\n",
      "level subproblems provably preserves the optimal solution of the original HAD-MFC. Experiments\n",
      "on large-scale MARL and rule-based systems show that our method outperforms baselines in 17 out\n",
      "of 18 tasks. It effectively identifies the most vulnerable agent, induces joint failures, and learns an\n",
      "interpretable value function that reveals agent vulnerabilities.\n",
      "Contributions. Our contributions are twofold. First, we address the robustness of large-scale MARL\n",
      "by proposing the problem of vulnerable agent identification (VAI), formulating it as a HAD-MFC,\n",
      "and analyzing its hardness. Second, we show that HAD-MFC can be solved by decomposing the\n",
      "hierarchical process into two separate problems via Fenchel-Rockafellar transform and solve the\n",
      "upper-level NP-hard problem via formulating it as a MDP with dense reward.\n",
      "2\n",
      "Related Work\n",
      "Learning Large-Scale MARL. In MARL, modeling the interactions between individual agents\n",
      "becomes impractical as the number of agents increases, making conventional MARL ineffective\n",
      "in large-scale [27]. Mean-Field Games (MFGs) [28, 17] offer a solution by modeling the overall\n",
      "distribution of agents, instead of individual agents. Recent advances in equilibrium learning for MFGs\n",
      "[29, 30, 4, 31, 32] have established strong theoretical foundations. Mean-Field Control (MFC) serves\n",
      "as the cooperative counterpart to MFGs [33, 34, 35]. Both frameworks assume a scenario where\n",
      "an infinite number of agents follow the same action distribution forming an mean field. However,\n",
      "in practical settings, agents need to take different actions based on their local states or specific\n",
      "policies. To address this, [1] extended the mean-field approximation to Markov games by modeling\n",
      "opponents through an action mean field using a Taylor expansion. This approach has been expanded\n",
      "to accommodate various MARL settings, including stationary [36], multi-type [37], and partially\n",
      "observable environments [38]. A more structured framework, known as decentralized MFGs [2],\n",
      "has also been developed, with significant contributions from [39, 40, 41]. Our study utilizes this\n",
      "decentralized framework, which has been proven to be highly effective in large-scale MARL [6].\n",
      "Adversarial Attacks for MARL. The goal of adversarial attacks for MARL is to develop worst-case\n",
      "adversarial attacks of MARL under uncertainties. This includes uncertainties in state [15, 21, 22, 23],\n",
      "action [42, 43], or environment [44, 45] to cause a well-trained MARL algorithm to fail during\n",
      "testing. Among these studies, several focus on selecting the most vulnerable agents to attack. For\n",
      "instance, GMA-FGSM [22] groups agents by their features and selects vulnerable agents based on\n",
      "their contribution to the total reward. ARTS [46] evaluates system robustness by repeatedly selecting\n",
      "random groups of agents to act as attackers. The work most similar to ours is RTCA [23], which\n",
      "employs a differential evolution algorithm to select vulnerable agents. However, these approaches\n",
      "are confined to small-scale MARL, and the challenge of scaling them to large-scale MARL remains\n",
      "unexplored.\n",
      "Influence Maximization. First proposed by [18], influence maximization involves selecting a set\n",
      "of nodes in a social network to influence the opinions of others through predefined rules. [18]\n",
      "demonstrated that this problem is NP-hard and introduced a greedy algorithm to solve it. Early\n",
      "works relied on heuristics, such as degree centrality [47, 48], graph structure [49, 50], genetic\n",
      "algorithms [51, 52], and community-based methods [53, 54]. More recent works address the problem\n",
      "by combining graph neural networks and reinforcement learning, learning a network embedding that\n",
      "serves as input to an RL algorithm for sequential node selection [55, 56, 57]. In contrast to these\n",
      "approaches, [58] demonstrated the potential to learn directly from network embeddings. However,\n",
      "most influence maximization studies assume a known graph, transition dynamics, and operate within\n",
      "a rule-based system. Our work does not rely on any of these assumptions.\n",
      "3\n",
      "\n",
      "3\n",
      "Problem Formulation\n",
      "3.1\n",
      "Hierarchical Adversarial Decentralized Mean-Field Control\n",
      "We formulate our problem as a Hierarchical Adversarial Mean-Field Control (HAD-MFC). To model\n",
      "large-scale MARL that assumes heterogeneous agents with mean-field approximations, we base our\n",
      "definition on decentralized Mean-Field Control (D-MFG) [2]. Next, HAD-MFC adapts D-MFG by\n",
      "fixing the victim policy and training an adversarial policy to (1) select a subset of agents from the\n",
      "victim agents (i.e., agents not being attacked) and (2) replace the selected agents’ policies with a\n",
      "worst-case adversarial policy. The HAD-MFC is defined as follows:\n",
      "G := ⟨N, S, A, P, R, µ0, ν0, γ⟩,\n",
      "where N = {1, . . . , N} represents the set of N agents, S and A denote the finite state and action\n",
      "spaces for each agent. P : S × A × ∆(S) × ∆(A) →∆(S) is the state transition probability\n",
      "function, R : S ×A×∆(S)×∆(A) →R is the shared reward function, µ0 ∈∆(S) and ν0 ∈∆(A)\n",
      "are the initial state and action distributions, and γ ∈[0, 1) is the discount factor. The interactions\n",
      "between agents are modeled through the mean-field state ∆(S) and action distribution ∆(A) in both\n",
      "the environment dynamics and rewards.\n",
      "Let T = {0, 1, . . . , T} represent the set of time steps. At t = 0, the attacker selects k agents to\n",
      "form an attack set K, where K ⊆N and |K| = k, which remains fixed in the episode. At each\n",
      "time step t ∈T , each agent i receives a local state si\n",
      "t ∈S and estimates the empirical mean-field\n",
      "state µt(s) = 1\n",
      "N\n",
      "P\n",
      "j∈N δ(sj\n",
      "t = s), with δ the Dirac’s delta. Each agent first executes a fixed, well-\n",
      "trained cooperative policy πβ(ai\n",
      "t|si\n",
      "t, µt) : S × ∆(S) →∆(A). To model the policy deviation under\n",
      "uncertainty, we assign a perturbation budget ϵi ∈[0, 1] for each agent. If agent i is in attack set K, the\n",
      "adversary learns an adversarial action perturbation policy πα(ai\n",
      "t|si\n",
      "t, µt) : S × ∆(S) →∆(A), and\n",
      "yields a perturbed policy ˆπi = ϵiπi\n",
      "α +(1−ϵi)πi\n",
      "β ∈∆(A), following the definition of PR-MDP in [9].\n",
      "If agent i is not in attack set K, the victim executes ˆπ = πβ with ϵi = 0. The empirical mean-field\n",
      "action is νt(a) = 1\n",
      "N\n",
      "P\n",
      "j∈N δ(aj\n",
      "t = a). The reward at time t is given by rt = R(si\n",
      "t, ai\n",
      "t, µt, νt), which\n",
      "is shared across agents. The game then transitions to time t + 1, generating a new local state for each\n",
      "agent based on the environment transition p(si\n",
      "t+1|si\n",
      "t, ai\n",
      "t, µt, νt). The expected reward is:\n",
      "J(ˆπ) ≡J(πα, πβ) = Eπα,πβ\n",
      "\" ∞\n",
      "X\n",
      "t=0\n",
      "γtR(si\n",
      "t, ai\n",
      "t, µt, νt)\n",
      "#\n",
      ".\n",
      "(1)\n",
      "Attacker’s goal. The attacker’s goal is to select an attack set K such that the agents in K learn an\n",
      "adversarial policy to minimize the expected reward:\n",
      "min\n",
      "K⊆N ,|K|=k min\n",
      "πα J(πα, πβ).\n",
      "(2)\n",
      "Complexity issue. The attacker face a hierarchical problem. The upper level face a combinatorial\n",
      "problem to select the k most vulnerable agents, and the lower level learns an adversarial policy for\n",
      "these selected agents. The coupled nature characterize the complexity issue of our problem.\n",
      "Relation to existing formulations. Our definition of HAD-MFC is distinct yet related to several\n",
      "existing formulations in the literature. Our study focus on control of practical large-scale MARL\n",
      "with mean-field approximation [2, 34] rather than theoretical MFGs and MFCs [29, 31, 33], and\n",
      "specifically focuses on the selection of vulnerable agents rather than equilibrium learning and optimal\n",
      "agent control. Our upper-level problem of selecting vulnerable agents is conceptually similar to\n",
      "influence maximization (IM) [18]. However, unlike IM, where influencing agents follow predefined\n",
      "rules, our framework requires agents to learn an adversarial policy and to cooperate optimally with\n",
      "other adversarial agents. Our lower-level problem is related to adversarial attacks in MARL [14].\n",
      "Existing works either do not involve the selection of vulnerable agents [15, 43], or are limited\n",
      "to small-scale settings [21, 23]. Our approach addresses adversarial attacks in large-scale MARL\n",
      "environments using mean-field approximations, which are significantly more complex than previously\n",
      "studied methods.\n",
      "4\n",
      "\n",
      "3.2\n",
      "Assumptions and Theoretical Analysis\n",
      "In this section, we outline the assumptions underlying our attack model. Building on existing\n",
      "studies on adversarial MARL [9, 14, 59, 16], we introduce a practical threat model based on specific\n",
      "assumptions regarding the capabilities of both victims and attackers at different levels.\n",
      "Assumption 3.1 (Victim’s capability). Victims follow a fixed, well-trained policy πβ that remains\n",
      "unchanged during the attack.\n",
      "We assume that the victim policies are fixed to simulate an attack scenario at test time, where the\n",
      "large-scale MARL system is deployed and its policy does not adapt in response to the attack [9, 14].\n",
      "We now describe the assumptions concerning the attackers.\n",
      "Assumption 3.2 (Upper-level attacker’s capabilities and limitations). The upper-level attacker can\n",
      "select k agents from N and assign them perturbation budget ϵi\n",
      "i∈K = ϵ only at the beginning of an\n",
      "episode. The upper-level attacker has access to all agents’ trajectories under the cooperative case,\n",
      "τ = [{si\n",
      "0}i∈N , {ai\n",
      "0}i∈N , µ0, ν0, r0, . . . , {si\n",
      "T }i∈N , {ai\n",
      "T }i∈N , µT , νT , rT ]. During the attack, it can\n",
      "also access the local state {si\n",
      "t}i∈N of all agents at t = 0 and the cumulative reward r = P\n",
      "t∈T γtrt.\n",
      "It does not have access to the policy parameters of the victim agents.\n",
      "Proposition 3.3 (Hardness). The problem faced by the upper-level attacker is NP-hard.\n",
      "Proof sketch. This is proven by reducing the problem to the generalized maximum coverage problem,\n",
      "which is NP-hard [24]. See full proof in Appendix. A.1.\n",
      "Assumption 3.4 (Lower-level attacker’s capabilities and limitations). The lower-level attacker\n",
      "minπα J(πα, πβ) has access to its local state si\n",
      "t, the empirical mean field µt, νt, and the reward rt.\n",
      "It does not have access to the policies, value functions, or local states of other agents.\n",
      "Our upper-level attacker only requires access to cooperative trajectory data, which is relatively easy\n",
      "to obtain. Furthermore, our attack model is black-box for both upper-level and lower-level, without\n",
      "the need of victim’s policy (note that for lower-level attacker, its policy is added on, yet irrelevant to\n",
      "victim policy). Lastly, we establish the existence of an optimal adversary.\n",
      "Proposition 3.5 (Existence of optimal adversary). For any HAD-MFC, there exists an optimal (i.e.,\n",
      "most harmful) upper-level adversary K and a corresponding lower-level adversary πα.\n",
      "Proof sketch. The upper-level attack is a finite combinatorial problem with an optimal solution. At\n",
      "the lower level, with fixed victim policies treated as part of the environment, the attacker solves\n",
      "a MFC problem with optimal solution. The optimal adversary exists by exploring all upper-level\n",
      "configurations and selecting the best lower-level policy. See full proof in Appendix. A.2.\n",
      "4\n",
      "Method\n",
      "In this section, we propose algorithms to solve the complexity issue of HAD-MFC. We begin by\n",
      "decoupling the hierarchical problem, eliminating the need to train a worst-case lower-level adversary\n",
      "by reformulating it into a regularized mean-field Bellman operator. We then formulate the upper-\n",
      "level combinatorial task as a MDP with dense reward computed from the value function from the\n",
      "regularized mean-field Bellman operator, and solve it via greedy algorithm or RL.\n",
      "4.1\n",
      "Decoupling the Hierarchical Problem\n",
      "Training the worst-case adversary πα is computationally expensive since it requires solving the RL\n",
      "problem minπα J(πα, πβ). To address this, we propose a regularized mean-field Bellman operator\n",
      "that efficiently estimates the value function under a worst-case adversary, using cooperative trajecto-\n",
      "ries only. Our approach involves defining the Bellman function for the adversary, characterizing the\n",
      "uncertainty set induced by πα, and applying Fenchel-Rockafellar transform to derive the solution.\n",
      "Bellman operators. To begin, we define the value function V i(si, µ) for our problem:\n",
      "V i(si, µ) = E\n",
      "\" ∞\n",
      "X\n",
      "t=0\n",
      "γtrt\n",
      "\f\f\f\fs0 = s, µ0 = µ, ai\n",
      "t ∼ˆπ(·|si\n",
      "t, µt)\n",
      "#\n",
      ".\n",
      "(3)\n",
      "The Bellman operator Bˆπ with victim and adversary policy can be defined as:\n",
      "(BˆπV i)(si, µ) =\n",
      "X\n",
      "a∈A\n",
      "ˆπ(ai|si, µ)ν(a)\n",
      "h\n",
      "r + γ\n",
      "X\n",
      "s′∈S\n",
      "p(s′i|si, ai, µ, ν)V (s′i, µ′)\n",
      "i\n",
      ".\n",
      "(4)\n",
      "5\n",
      "\n",
      "With worst-case adversary, we can further define the worst-case Bellman operator as:\n",
      "( ˆBˆπV i)(si, µ) = min\n",
      "πα (BˆπV i)(si, µ)\n",
      "(5)\n",
      "Uncertainty set characterization. We proceed by characterizing the impact of πα on perturbed\n",
      "policy ˆπ and the perturbed mean-field action ν(a). We expand them as:\n",
      "ˆπi = ϵiπi\n",
      "α + (1 −ϵi)πi\n",
      "β, lim\n",
      "N→∞ν(a) = ξνα(a) + (1 −ξ)νβ(a),\n",
      "where\n",
      "ξ = 1\n",
      "N\n",
      "X\n",
      "i∈N\n",
      "ϵi, να(a) = 1\n",
      "N\n",
      "X\n",
      "i∈N\n",
      "δ(ai\n",
      "t = a|πα), νβ(a) = 1\n",
      "N\n",
      "X\n",
      "i∈N\n",
      "(1 −ϵi)δ(ai\n",
      "t = a|πβ). (6)\n",
      "We can then derive the uncertainty set induced by πα:\n",
      "Proposition 4.1. The difference of perturbed policy and victim policy, as well as perturbed mean-field\n",
      "action and victim mean-field action can be (approximately) bounded in ℓp norm:\n",
      "||ˆπi −πi\n",
      "β||p ≤21/pϵi, p\n",
      "\u0000\f\f||ν(a) −νβ(a)||p −21/pξ\n",
      "\f\f ≥δ\n",
      "\u0001\n",
      "≤2 exp\n",
      "\u0000−2Nδ2\u0001\n",
      ", ∀δ > 0.\n",
      "(7)\n",
      "Proof sketch. The proof for ˆπ is by expanding itself and ||πα −πβ||p ≤21/p. The proof for ν is\n",
      "by Jensen’s inequality and the probability is by Hoeffding’s inequality. To avoid overly cluttered\n",
      "expressions, we drop the constant 21/p and simplify the expression as ||ˆπi −πi\n",
      "β||p ≤ϵi and\n",
      "||ν(a) −νβ(a)|| ⪅ξ in subsequent derivations. See full proof in Appendix.A.3.\n",
      "Fenchel-Rockafellar transform. With uncertainty set defined, we simplify the notation by ˆπi\n",
      "α =\n",
      "ˆπi −πi\n",
      "β and ˆνα(a) = ν(a) −νβ(a), which is bounded by ||ˆπi\n",
      "α||p ≤ϵi and ˆνα(a) ⪅ξ by Proposition.\n",
      "4.1. We proceed by expanding the Bellman equation in Eqn. 5:\n",
      "(BˆπV i)(si, µ) =\n",
      "X\n",
      "ai,a∈A\n",
      "\u0000ˆπi\n",
      "α + πi\n",
      "β\n",
      "\u0001\n",
      "(ˆνα(a) + νβ(a))\n",
      "h\n",
      "rt + γ\n",
      "X\n",
      "s′∈S\n",
      "p(s′i|si, ai, µ, ν)V (s′i, µ′)\n",
      "i\n",
      ".\n",
      "(8)\n",
      "As we have proved in Proposition 3.5, an optimal adversary always exists. With (BˆπV i)(si, µ) =\n",
      "minπα(BˆπV i)(si, µ), we then have the following robust feasible inequality [60]:\n",
      "V i(si, µ) = (BˆπV i)(si, µ) ≤( ˆBˆπV i)(si, µ),\n",
      "V i(si, µ) −( ˆBˆπV i)(si, µ) ≤0,\n",
      "(9)\n",
      "with equality holds when πα reach optimality π∗\n",
      "α. Thus, we are solving the following problem via\n",
      "Fenchel-Rockafellar transform [25, 26]:\n",
      "max\n",
      "πα V i(si, µ) −( ˆBˆπV i)(si, µ).\n",
      "(10)\n",
      "Proposition 4.2. The Fenchel-Rockafellar transform of Eqn. 10 results in a regularized mean-field\n",
      "Bellman operator BR\n",
      "ϵi,ξ:\n",
      "max\n",
      "πα V i(si, µ) −( ˆBˆπV i)(si, µ) = V i(si, µ) −BR\n",
      "ϵi,ξV i(si, µ, ϵi, ξ)\n",
      "= V i(si, µ) −(BπβV i)(si, µ) + (ϵi + ξ + ϵiξ)||Qi(si, ai, µ, ν)||q.\n",
      "(11)\n",
      "Here, 1/p + 1/q = 1 is the dual of ℓp norm via Fenchel-Rockafellar transform. In this way, our\n",
      "learned value function V i(si, µ, ϵi, ξ) estimated from our Bellman estimator BR\n",
      "ϵi,ξ accounts for the\n",
      "value of agent i under attack, condition on current situation whether (1) ϵi, which indicates if agent\n",
      "i itself is perturbed, and (2) the mean-field approximation on ξ, the number of its teammates gets\n",
      "perturbed.\n",
      "Proof sketch. We first expand ˆπ and ν(a) in Eqn. 5, resulting in a Q function with uncertainty.\n",
      "Applying Fenchel-Rockafellar transform completes the proof. See full proof in Appendix. A.4.\n",
      "Proposition 4.3 (Contraction). The regularized mean-field Bellman operator BR\n",
      "ϵi,ξV i(si, µ, ϵi, ξ) =\n",
      "(BπβV i)(si, µ) + (ϵi + ξ + ϵiξ)||Qi(si, ai, µ, ν)||q is a contraction operator.\n",
      "Proof sketch. To proof that, we find ||Qi(si, ai, µ, ν)||q term cancels each other and the rest follows\n",
      "standard approach. See full proof in Appendix. A.5.\n",
      "6\n",
      "\n",
      "Proposition 4.4 (Relation to worst-case Q function). To understand our Bellman operator, we show\n",
      "ϵiξ||Qi(si, ai, µ, ν)||q is identical to the gap between the cooperative and worst-case Q function\n",
      "under ℓ1 norm bounded perturbed action ai\n",
      "α and mean-field action να induced by πα:\n",
      "ϵiξ||Qi(si, ai, µ, ν)||q =\n",
      "max\n",
      "||aiα||p≤ϵi,||να||p≤ξ ||Qi(si, ai, µ, ν) −Qi(si, (ai + ai\n",
      "α), µ, (ν + να))||1. (12)\n",
      "Proof sketch. The proof is done by first making a linear approximation of Q function, then applying\n",
      "Hölder’s inequality. See full proof in Appendix. A.6.\n",
      "Remark 1. The regularization terms in BR arises from uncertainties in agents and the mean-field.\n",
      "To clarify, the term ϵi||Qi(si, ai, µ, ν)||q capture agent vulnerability, ξ||Qi(si, ai, µ, ν)||q capture\n",
      "mean-field vulnerability, and ϵiξ||Qi(si, ai, µ, ν)||q capture vulnerability of their interactions. Each\n",
      "term yields more pessimistic value estimation when there are larger uncertainties in its actions,\n",
      "mean-field, or their interactions.\n",
      "Remark 2. Notably, our approach does not assume πβ to be optimal, which means it can be extended\n",
      "to agent-based systems governed by predefined rules [61], provided these rules can be derived from\n",
      "Q-functions (e.g., using a Boltzmann-based policy).\n",
      "4.2\n",
      "Algorithm for Vulnerable Agent Identification\n",
      "In this section, we give a practical algorithm to solve upper-level vulnerable agent identification. We\n",
      "formulate this NP-hard problem as a MDP with dense reward calculated by regularized mean-field\n",
      "Bellman operator. We next propose RL and greedy algorithm for solving this MDP. Finally, we prove\n",
      "the decomposition of HAD-MFC do not alter the optimal solution.\n",
      "Problem formulation. The problem faced by the upper-level adversary can be formulated as a\n",
      "Markov Decision Process, defined based on HAD-MFC:\n",
      "M := ⟨S, ϵ, N, ˜P, ˜R, γ⟩,\n",
      "where S = ×i∈N Si is the local state space of each agent. The game proceeds in K steps, with K\n",
      "the number of adversaries we select. At step k, ϵk ∈[0, 1]N = {ϵi\n",
      "k}i∈N is the perturbation budget of\n",
      "each agent at step k, with ϵi\n",
      "0 = 0, ∀i ∈N. N is the action space, where agents could be selected as\n",
      "vulnerable agent, ˜P : S × N →S is the state transition, and ˜R : S × N × [0, 1] →R is the reward\n",
      "function, γ is the discount factor. At each step k, we select the most vulnerable agent n, and update\n",
      "the value of ϵk. Note that if we merge ϵ in S, the problem becomes a standard MDP.\n",
      "Reward. Reward specifies the objective of MDP. In our case, the reward is defined as: given the set\n",
      "of selected vulnerable agents Kk−1 and the new selected agent nk at step k, what is the amount of\n",
      "reward the victim large-scale MARL system going to decrease, had it face the worst-case adversary\n",
      "trained on this new set of selected vulnerable agents Kk = Kk−1 ∪nk?\n",
      "To calculate this value efficiently, we resort to the regularized mean-field Bellman operator BR\n",
      "ϵi,ξ\n",
      "in Eqn. 4.2, which defines the amount of reward we expected to receive, given the ℓp bounded\n",
      "perturbation magnitude ϵi\n",
      "k and ξk at step k. Define the value function learned under BR\n",
      "ϵi,ξ at time\n",
      "t = 0 as V i(si\n",
      "0, µ0, ϵi\n",
      "k, ξk), the reward can then be defined as:\n",
      "rk = ˜R(sk, ϵk, nk) = 1\n",
      "N\n",
      "X\n",
      "i∈N\n",
      "\u0000V i(si\n",
      "0, µ0, ϵi\n",
      "k, ξk) −V i(si\n",
      "0, µ0, ϵi\n",
      "k−1, ξk−1)\n",
      "\u0001\n",
      ".\n",
      "(13)\n",
      "Here ϵi\n",
      "k can take any values between [0, 21/p] and ξk depends on ϵi\n",
      "k. We thus define the TD loss as:\n",
      "min Eτ∼πβ(V i(si, µ, ϵi, ξ) −r −γV i(s′i, µ′, ϵi, ξ) + (ϵiξ + ϵi + ξ)||Qi(si, ai\n",
      "β, µ, νβ)||q)2, (14)\n",
      "with ϵ ∼Uniform[0, 21/p], ξ ∼Bernouli(ξ). This can be optimized using sampled trajectory in\n",
      "cooperative case (i.e., τ ∼πβ), which can be easy to obtain.\n",
      "Solving the MDP. Given the RL formulation, we can optimize our VAI problem using any RL\n",
      "algorithm, such as Q-learning [62]. The Q function can be updated via standard TD loss. We call this\n",
      "approach as VAI-RL. Alternatively, the reward defined in Eqn. 13 suggests a fast greedy algorithm,\n",
      "such that the agent to maximize reward is selected at each step. We call this approach VAI-Greedy.\n",
      "We include both algorithms for comparison. See the pseudo code of both methods in Appendix. B.\n",
      "7\n",
      "\n",
      "Proposition 4.5 (Optimality of Decomposition). Given a HAD-MFC G := ⟨N, S, A, P, R, µ0, ν0,\n",
      "γ⟩. For the upper-level MDP M := ⟨S, ϵ, N, ˜P, ˜R, γ⟩with reward defined in Eqn. 13, and the value\n",
      "V i,∗(si, µ, ϵi, ξ) of lower-level problem is learned by regularized mean-field Bellman operator BR\n",
      "ϵi,ξ,\n",
      "define the optimal vulnerable agents of M as K∗⊆N. The selected vulnerable agents K∗⊆N and\n",
      "the worst-case adversarial policy learned π∗\n",
      "α under K∗⊆N is the optimal solution of HAD-MFC.\n",
      "Proof sketch. The optimality of lower-level policy π∗\n",
      "α is done by the property of Rockfellar-Fenchel\n",
      "transform. The optimality of higher-level problem follows the fact that optimal solution exists in\n",
      "MDP. See full proof in Appendix. A.7.\n",
      "5\n",
      "Experiments\n",
      "Environments. We evaluate our algorithms in three environments: Battle [6], Taxi Matching [8], and\n",
      "Vicsek [63]. The Vicsek environment is used to test our algorithm in rule-based systems. Detailed\n",
      "descriptions of the environments are provided in Appendix C.1. We train all victim agents in Battle\n",
      "using MF-Q, and Taxi Matching using MF-AC [1], which empirically yields better task performance.\n",
      "Baselines. To our knowledge, the problem of vulnerable agent identification in MARL is rarely\n",
      "studied in literature. Therefore, we select five relevant studies as baselines: (1) Random selection,\n",
      "serving as a simple baseline. (2) Degree centrality (DC) [64], a heuristic method that select agents\n",
      "with the most connections with others. (3) Bi-level RL [65], which trains our upper-level and lower-\n",
      "level problems hierarchically. (4) PIANO [56], which selects critical agents iteratively via graph\n",
      "embeddings and RL. (5) RTCA [23], which selects vulnerable agents in small-scale MARL using\n",
      "differential evolution. For methods requiring a graph structure, we construct an undirected graph with\n",
      "an edge of weight 1 between two agents if they can observe each other, and 0 otherwise. We call our\n",
      "method as Vulnerable Agent Identification (VAI). All baselines are trained using the same codebase,\n",
      "network structure, and hyperparameters to ensure fair comparison. Detailed implementations and\n",
      "hyperparameters are provided in Appendices C.2 and C.3.\n",
      "Evaluation protocol. We consider {ϵi}i∈K = 1 bounded by ℓ∞norm. The setting allows adversaries\n",
      "to manipulate the policy of πβ arbitrarily. The scenario occurs when agents crash in the environment,\n",
      "or are compromised by the adversary [10, 11, 14]. Results of different ϵ in Appendix. D.2. The\n",
      "number of attackers, K, is empirically determined based on the total number of agents in the\n",
      "environment. We report the results on victims and attackers with five random seeds.\n",
      "5.1\n",
      "Simulation Results\n",
      "Table 1: Performance of our VAI-RL and VAI-Greedy in three environments, each with two map sizes\n",
      "and different numbers of attackers. Our VAI methods consistently achieve better attack capability.\n",
      "Environment: Battle (↓)\n",
      "Agent Num Adv. Num\n",
      "Random\n",
      "DC\n",
      "Bi-Level RL\n",
      "PIANO\n",
      "RTCA\n",
      "VAI-Greedy\n",
      "VAI-RL\n",
      "64\n",
      "8\n",
      "298.47±76.56\n",
      "305.16±45.39\n",
      "295.09±12.96\n",
      "296.79±47.67\n",
      "301.08±22.72\n",
      "287.53±9.39\n",
      "281.50±17.33\n",
      "16\n",
      "97.33±34.52\n",
      "93.54±34.56\n",
      "87.37±6.28\n",
      "81.06±11.34\n",
      "85.71±24.62\n",
      "72.01±20.28\n",
      "77.73±1.81\n",
      "32\n",
      "−152.89±26.75\n",
      "−160.51±75.32\n",
      "−198.03±55.83\n",
      "−175.24±39.11\n",
      "−192.78±43.81\n",
      "−214.40±43.12\n",
      "−929.88±62.73\n",
      "144\n",
      "18\n",
      "730.65±117.42\n",
      "693.15±98.87\n",
      "685.77±124.51\n",
      "670.55±66.75\n",
      "650.33±50.47\n",
      "610.62±31.36\n",
      "505.34±30.79\n",
      "36\n",
      "250.43±120.19\n",
      "140.67±76.67\n",
      "189.95±15.54\n",
      "130.63±34.69\n",
      "155.02±170.74\n",
      "85.52±35.11\n",
      "86.26±38.72\n",
      "72\n",
      "−1809.01±130.98 −2014.57±670.92 −2353.78±870.53 −2313.46±230.66 −2221.12±360.49 −2579.80±256.19 −2837.83±482.56\n",
      "Environment: Taxi (↓)\n",
      "Agent Num Adv. Num\n",
      "Random\n",
      "DC\n",
      "Bi-Level RL\n",
      "PIANO\n",
      "RTCA\n",
      "VAI-Greedy\n",
      "VAI-RL\n",
      "50\n",
      "4\n",
      "33.9±14.39\n",
      "19.07±5.77\n",
      "27.52±16.12\n",
      "23.55±7.44\n",
      "16.26±3.32\n",
      "10.47±4.85\n",
      "12.47±8.73\n",
      "16\n",
      "109.94±7.32\n",
      "79.01±11.33\n",
      "162.23±2.31\n",
      "140.60±49.01\n",
      "138.73±1.72\n",
      "54.63±8.81\n",
      "64.72±3.76\n",
      "36\n",
      "617.09±51.80\n",
      "595.80±60.28\n",
      "571.26±59.96\n",
      "516.91±44.86\n",
      "618.21±54.08\n",
      "463.70±55.99\n",
      "365.96±63.75\n",
      "100\n",
      "4\n",
      "34.49±22.61\n",
      "21.17±3.47\n",
      "14.17±3.07\n",
      "36.51±6.11\n",
      "16.87±8.27\n",
      "8.27±8.67\n",
      "4.95±2.86\n",
      "16\n",
      "172.00±75.41\n",
      "141.19±5.80\n",
      "201.14±68.66\n",
      "202.51±47.18\n",
      "140.76±32.44\n",
      "153.97±8.52\n",
      "186.62±40.79\n",
      "36\n",
      "884.49±68.87\n",
      "867.62±23.46\n",
      "892.51±66.15\n",
      "793.71±12.86\n",
      "860.58±106.61\n",
      "770.14±29.74\n",
      "652.10±23.23\n",
      "Environment: Vicsek (↑)\n",
      "Agent Num Adv. Num\n",
      "Random\n",
      "DC\n",
      "Bi-Level RL\n",
      "PIANO\n",
      "RTCA\n",
      "VAI-Greedy\n",
      "VAI-RL\n",
      "100\n",
      "20\n",
      "−226.96±11.54\n",
      "−232.45±3.77\n",
      "−221.26±14.06\n",
      "−250.83±19.59\n",
      "−225.12±28.05\n",
      "−167.60±3.91\n",
      "−183.68±19.56\n",
      "35\n",
      "−159.83±40.85\n",
      "−143.14±42.37\n",
      "−141.51±43.28\n",
      "−162.74±28.45\n",
      "−129.24±13.30\n",
      "−113.64±15.78\n",
      "−93.65±28.65\n",
      "50\n",
      "−96.83±7.26\n",
      "−95.22±6.19\n",
      "−96.80±0.76\n",
      "−86.21±3.55\n",
      "−82.63±5.70\n",
      "−70.52±5.21\n",
      "−75.82±2.57\n",
      "400\n",
      "80\n",
      "−884.34±53.96\n",
      "−840.87±33.67\n",
      "−780.31±90.02\n",
      "−950.13±110.36\n",
      "−872.21±130.11\n",
      "−710.56±56.32\n",
      "−659.65±86.73\n",
      "140\n",
      "−480.17±50.16\n",
      "−440.63±80.67\n",
      "−460.43±74.71\n",
      "−510.24±62.11\n",
      "−410.14±87.33\n",
      "−390.74±42.16\n",
      "−302.76±76.37\n",
      "200\n",
      "−295.13±36.94\n",
      "−313.55±49.43\n",
      "−310.78±56.89\n",
      "−290.53±27.89\n",
      "−287.53±46.76\n",
      "−256.44±21.34\n",
      "−275.62±37.76\n",
      "8\n",
      "\n",
      "First, we evaluate the effectiveness of our method on finding the most vulnerable agents to attack.\n",
      "This is done by (1) solve the upper-level problem of finding the most vulnerable agents and (2)\n",
      "solve the lower-level problem of learning a worst-case policy for these vulnerable agents. For\n",
      "comprehensiveness, for each task, we evaluate them on six subtasks, including two map sizes with\n",
      "different agent numbers in the game, and each map sizes with three different number of adversaries.\n",
      "As shown in Table. 1, our VAI based method outperforms all baselines in 17 out of 18 tasks, while\n",
      "heuristic-based method and learning based method are only slightly better than random selection.\n",
      "To explain this, heuristic-based method such as degree centrality (DC) are designed for rule-based\n",
      "systems. However, in large-scale MARL, the interaction between agents are nonlinear and are not\n",
      "clearly defined by rules. For example, in Battle environment, agents in the center of the crowd are\n",
      "less susceptible to attack than the agents in frontline, combating enemies, making DC ineffective. As\n",
      "for learning-based method, PIANO do not account for the worst-case policy made by agents, thus are\n",
      "unable to select the set of most harmful agents under adversarial policies. Solving our problem via\n",
      "bi-level RL and RTCA do not work well due to the hierarchical nature of our problem, which may be\n",
      "too hard for RL to solve without any guidance. In contrast, our VAI method works well due to the the\n",
      "more accurate value function we learned via Bellman operator BR\n",
      "ϵi,ξ.\n",
      "Additionally, we observed that VAI-RL outperforms VAI-Greedy in 10 of 18 tasks, especially when\n",
      "more attackers are available. To explain, Greedy algorithm focuses on immediate reward and works\n",
      "well with less attackers and weak agent-wise interactions. RL, in contrast, models long-term returns\n",
      "and inter-agent impact, which performs better with more attackers. Additionally, RL provides\n",
      "theoretical guarantees for optimality in MDPs, which greedy methods lack.\n",
      "0\n",
      "100 200 300 400 500 600 700\n",
      "Reward Under Attack\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Predicted Value\n",
      "Pearson: R=0.97, p-value = 1.85e-11\n",
      "Task: Battle, Agent Num 64\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "Reward Under Attack\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "1.5\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "3.5\n",
      "Predicted Value\n",
      "Pearson: R=0.91, p-value = 1.61e-07\n",
      "Task: Taxi, Agent Num 100\n",
      "Figure 1: Pearson correlation between the lower-level attack\n",
      "value estimated by our Bellman operator BR\n",
      "ϵi,ξ and lower-\n",
      "level reward by running an attack using RL.\n",
      "Finally, we would like to highlight\n",
      "that our VAI algorithm yields superior\n",
      "results not only on MARL environ-\n",
      "ments, but also on rule-based Vicsek\n",
      "environment. This is done by approx-\n",
      "imating a value function using col-\n",
      "lected rule-based trajectories, then es-\n",
      "timate the vulnerability of each agent\n",
      "via our proposed Bellman operator\n",
      "BR\n",
      "ϵi,ξ. In this end, we expect our work\n",
      "to have future impacts in rule-based\n",
      "complex systems with real-world im-\n",
      "pact, such as social networks [18, 19].\n",
      "Computational Efficiency: While our VAI requires training an additional function in Proposition\n",
      "4.2, the overall computation cost is on par with our baselines. See full results in Appendix. D.1.\n",
      "Results with Different ϵ: Under different perturbation budget ϵ, our VAI-RL and VAI-Greedy\n",
      "consistently outperform other baselines. See full results in Appendix. D.2.\n",
      "5.2\n",
      "Discussions and Insights\n",
      "In this section, we thoroughly evaluate the effectiveness of our method, showing our theory is effective\n",
      "in practice and our method offers meaningful insights to the robustness of large-scale MARL.\n",
      "Our Method is Effective by Value Estimation. We attribute the effectiveness of our methods via\n",
      "the regularized mean-field Bellman operator BR\n",
      "ϵi,ξ in Proposition. 4.2. To verify this, we compare the\n",
      "results predicted by our value function of the lower-level attack, and the reward gained by actually\n",
      "running the lower-level attack via RL. As shown in Fig. 1, we find the value function learned by BR\n",
      "ϵi,ξ\n",
      "is effective at predicting the attack result of the worst-case adversarial policy for vulnerable agent\n",
      "selections, showing strong Pearson correlation (r = 0.97 for Battle, r = 0.91 for Taxi, p < .001).\n",
      "Thus, BR\n",
      "ϵi,ξ effectively decompose HAD-MFC by acting as a predictor of lower-level attack.\n",
      "Certain agents are more vulnerable than others. In large-scale MARL systems, some agents\n",
      "play disproportionately critical roles, making them inherently more vulnerable. To illustrate this, we\n",
      "visualize agent values in the Battle-64 and Taxi-100 environments using heatmaps in Fig. 2, where\n",
      "each cell represents the importance of a single agent at the start of the game. In Battle-64, 64 agents\n",
      "are arranged in an 8×8 grid to engage with another team of 64 agents; we display only the 64 agents\n",
      "9\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "x axis\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "y axis\n",
      "3.45\n",
      "0.62\n",
      "0.53\n",
      "1.51\n",
      "0.42\n",
      "1.49\n",
      "0.73\n",
      "1.26\n",
      "1.81\n",
      "1.01\n",
      "1.79\n",
      "1.31\n",
      "1.15\n",
      "0.81\n",
      "0.70\n",
      "1.90\n",
      "3.10\n",
      "1.03\n",
      "1.07\n",
      "0.67\n",
      "1.37\n",
      "6.46\n",
      "0.97\n",
      "1.04\n",
      "1.69\n",
      "4.60\n",
      "0.56\n",
      "2.34\n",
      "0.66\n",
      "2.52\n",
      "0.87\n",
      "1.08\n",
      "3.33\n",
      "2.05\n",
      "1.99\n",
      "1.73\n",
      "0.92\n",
      "5.03\n",
      "3.47\n",
      "0.86\n",
      "3.30\n",
      "0.65\n",
      "2.95\n",
      "2.38\n",
      "1.88\n",
      "0.79\n",
      "1.43\n",
      "2.90\n",
      "-0.04\n",
      "-0.35\n",
      "1.88\n",
      "1.81\n",
      "0.27\n",
      "4.93\n",
      "0.47\n",
      "2.45\n",
      "3.35\n",
      "1.01\n",
      "1.25\n",
      "9.14\n",
      "3.08\n",
      "2.93\n",
      "2.35\n",
      "3.75\n",
      "Task:Battle,Agent Num 64\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "Value\n",
      "(a) Battle (ϵi = 1)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "x axis\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "y axis\n",
      " 194\n",
      "0\n",
      "451\n",
      "352\n",
      "275\n",
      "960\n",
      "965\n",
      "374\n",
      "-100\n",
      "379\n",
      "618\n",
      "159\n",
      "748\n",
      "636\n",
      "404\n",
      "1092\n",
      "1082\n",
      "965\n",
      "357\n",
      "625\n",
      "662\n",
      "1200\n",
      "749\n",
      "1454\n",
      "1455\n",
      "996\n",
      "811\n",
      "1019\n",
      "993\n",
      "211\n",
      "699\n",
      "1259\n",
      "1260\n",
      "1689\n",
      "1850\n",
      "1338\n",
      "346\n",
      "506\n",
      "820\n",
      "251\n",
      "1289\n",
      "1622\n",
      "1770\n",
      "2019\n",
      "2024\n",
      "1743\n",
      "1110\n",
      "1295\n",
      "1150\n",
      "412\n",
      "746\n",
      "1340\n",
      "1497\n",
      "1689\n",
      "1561\n",
      "1482\n",
      "1731\n",
      "882\n",
      "817\n",
      "20\n",
      "1249\n",
      "902\n",
      "1759\n",
      "1676\n",
      "1594\n",
      "1458\n",
      "1387\n",
      "870\n",
      "1144\n",
      "374\n",
      "775\n",
      "1233\n",
      "1519\n",
      "1749\n",
      "1735\n",
      "1745\n",
      "1618\n",
      "906\n",
      "666\n",
      "264\n",
      "499\n",
      "466\n",
      "1208\n",
      "1147\n",
      "571\n",
      "1654\n",
      "928\n",
      "537\n",
      "429\n",
      "-12\n",
      "274\n",
      "Task:Taxi,Agent Num 100\n",
      "0\n",
      "250\n",
      "500\n",
      "750\n",
      "1000\n",
      "1250\n",
      "1500\n",
      "1750\n",
      "2000\n",
      "Value\n",
      "-193\n",
      "198\n",
      "279\n",
      "192\n",
      "220\n",
      "1052\n",
      "1225\n",
      "455\n",
      "181\n",
      "(b) Taxi (ϵi = 1)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "x axis\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "y axis\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.58\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.65\n",
      "0.80\n",
      "0.63\n",
      "0.64\n",
      "0.47\n",
      "0\n",
      "0\n",
      "0\n",
      "0.63\n",
      "0.86\n",
      "0.63\n",
      "0.48\n",
      "0.62\n",
      "0\n",
      "0\n",
      "0.93\n",
      "0.85\n",
      "0.80\n",
      "Adv\n",
      "0.88\n",
      "0.88\n",
      "0.86\n",
      "0\n",
      "Task:Battle,Agent Num 64\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "Value\n",
      "(c) Battle (ξ = 1/N)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "x axis\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "y axis\n",
      "192\n",
      "209\n",
      "241\n",
      "218\n",
      "268\n",
      "257\n",
      "187\n",
      "181\n",
      "204\n",
      "198\n",
      "210\n",
      "212\n",
      "220\n",
      "204\n",
      "265\n",
      "263\n",
      "212\n",
      "202\n",
      "205\n",
      "215\n",
      "213\n",
      "246\n",
      "213\n",
      "236\n",
      "Adv\n",
      "193\n",
      "213\n",
      "217\n",
      "198\n",
      "189\n",
      "213\n",
      "235\n",
      "166\n",
      "150\n",
      "137\n",
      "144\n",
      "158\n",
      "256\n",
      "236\n",
      "210\n",
      "251\n",
      "191\n",
      "167\n",
      "146\n",
      "143\n",
      "127\n",
      "96\n",
      "221\n",
      "183\n",
      "199\n",
      "219\n",
      "228\n",
      "184\n",
      "142\n",
      "161\n",
      "123\n",
      "130\n",
      "163\n",
      "218\n",
      "185\n",
      "241\n",
      "226\n",
      "215\n",
      "161\n",
      "163\n",
      "156\n",
      "132\n",
      "255\n",
      "222\n",
      "224\n",
      "231\n",
      "238\n",
      "172\n",
      "161\n",
      "153\n",
      "170\n",
      "113\n",
      "185\n",
      "242\n",
      "236\n",
      "201\n",
      "232\n",
      "249\n",
      "214\n",
      "203\n",
      "255\n",
      "151\n",
      "221\n",
      "231\n",
      "209\n",
      "187\n",
      "226\n",
      "238\n",
      "253\n",
      "227\n",
      "205\n",
      "230\n",
      "205\n",
      "221\n",
      "202\n",
      "Task:Taxi,Agent Num 100\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "Value\n",
      "(d) Taxi (ξ = 1/N)\n",
      "Figure 2: (a,b) Some agents contributes more to overall system when compromised, reflecting the\n",
      "change from ϵi = 0 to ϵi = 1. (c,d) Agents receive more impact when facing attackers, reflecting the\n",
      "change from ξ = 0 to ξ = 1/N. Darker cell indicates agents are more vulnerable under attacks.\n",
      "controlled by the mean-field MARL policy. In Taxi-100, 100 agents are uniformly positioned across\n",
      "a 10×10 map. The heatmaps reveal two key factors on vulnerability:\n",
      "First, some agents contribute more significantly to overall system functionality. Figs. 2a and 2b\n",
      "visualize the value difference V i(si, µ, ϵi = 0, ξ = 0) −V i(si, µ, ϵi = 1, ξ = 0), reflecting the drop\n",
      "in value if agent i is selected as an adversary, as captured by the ϵi term in Proposition 4.2. In Battle,\n",
      "agents at the right hand side engage enemies more frequently and thus accumulate more rewards,\n",
      "making them both more valuable and more vulnerable when compromised. In Taxi, ride requests\n",
      "occur more frequently near the center, so agents located there earn higher rewards and are similarly\n",
      "more critical. These patterns indicate that agents with advantageous positions or key roles contribute\n",
      "more to cooperation and are therefore prime targets for adversarial exploitation.\n",
      "Second, the failure of one agent can negatively affect others. Figs. 2c and 2d show the impact\n",
      "of a single adversary (highlighted in a red square) on its teammates’ value functions, computed as\n",
      "V i(si, µ, ϵi = 0, ξ = 0)−V i(si, µ, ϵi = 0, ξ = 1/N), corresponding to the ξ term in Proposition 4.2.\n",
      "In Battle, disruption primarily affects agents in the same row, where coordination is essential for\n",
      "collective attacks. In Taxi, agents to the left suffer most, as they must move toward the central region,\n",
      "but are blocked by the adversary. In contrast, central agents remain largely unaffected. These results\n",
      "demonstrate that the learned V i function captures inter-agent dependencies and accurately reflects\n",
      "vulnerability propagation within the team.\n",
      "6\n",
      "Conclusions\n",
      "In this paper, we evaluate the extent to which the failure of a group of agents adopting worst-case\n",
      "policies impacts the robustness of large-scale MARL. We define this problem as Vulnerable Agent\n",
      "Identification (VAI) and formulate it as a HAD-MFC. In this hierarchical framework, the upper level\n",
      "addresses the NP-hard problem of selecting the most vulnerable agents, while the lower level learns\n",
      "worst-case adversarial policies. We disentangle this hierarchical problem using Fenchel-Rockafellar\n",
      "transform and solve the NP-hard upper-level problem with greedy algorithm and RL. Experiments\n",
      "show that our method identifies groups of vulnerable agents in both large-scale MARL and rule-based\n",
      "systems, causing these systems to experience joint failures, and learns a value function that accurately\n",
      "predicts the vulnerability of each agent. Future work will focus on extending our method to real-world\n",
      "systems with graph structures, such as social networks. While misuse is theoretically possible, it is\n",
      "mitigated by the method’s greater utility to defenders and its reliance on trajectory data.\n",
      "References\n",
      "[1] Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field\n",
      "multi-agent reinforcement learning. In International conference on machine learning, pages\n",
      "5571–5580. PMLR, 2018.\n",
      "[2] Sriram Ganapathi Subramanian, Matthew E Taylor, Mark Crowley, and Pascal Poupart. Decen-\n",
      "tralized mean field games. In Proceedings of the AAAI Conference on Artificial Intelligence,\n",
      "volume 36, pages 9439–9447, 2022.\n",
      "[3] Barna Pasztor, Ilija Bogunovic, and Andreas Krause. Efficient model-based multi-agent mean-\n",
      "field reinforcement learning. arXiv preprint arXiv:2107.04050, 2021.\n",
      "10\n",
      "\n",
      "[4] Mathieu Laurière, Sarah Perrin, Sertan Girgin, Paul Muller, Ayush Jain, Theophile Cabannes,\n",
      "Georgios Piliouras, Julien Pérolat, Romuald Elie, Olivier Pietquin, et al. Scalable deep rein-\n",
      "forcement learning algorithms for mean field games. In International Conference on Machine\n",
      "Learning, pages 12078–12095. PMLR, 2022.\n",
      "[5] Maximilian Hüttenrauch, Sosic Adrian, Gerhard Neumann, et al. Deep reinforcement learning\n",
      "for swarm systems. Journal of Machine Learning Research, 20(54):1–31, 2019.\n",
      "[6] Lianmin Zheng, Jiacheng Yang, Han Cai, Ming Zhou, Weinan Zhang, Jun Wang, and Yong Yu.\n",
      "Magent: A many-agent reinforcement learning platform for artificial collective intelligence. In\n",
      "Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\n",
      "[7] Jianhong Wang, Wangkun Xu, Yunjie Gu, Wenbin Song, and Tim C Green. Multi-agent\n",
      "reinforcement learning for active voltage control on power distribution networks. Advances in\n",
      "Neural Information Processing Systems, 34:3271–3284, 2021.\n",
      "[8] Duc Thien Nguyen, Akshat Kumar, and Hoong Chuin Lau. Credit assignment for collective\n",
      "multiagent rl with global rewards. Advances in neural information processing systems, 31,\n",
      "2018.\n",
      "[9] Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and\n",
      "applications in continuous control. In International Conference on Machine Learning, pages\n",
      "6215–6224. PMLR, 2019.\n",
      "[10] Eliahu Khalastchi and Meir Kalech. Fault detection and diagnosis in multi-robot systems: A\n",
      "survey. Sensors, 19(18):4019, 2019.\n",
      "[11] Xinge Huang, Farshad Arvin, Craig West, Simon Watson, and Barry Lennox. Exploration in\n",
      "extreme environments with swarm robotic system. In 2019 IEEE international conference on\n",
      "mechatronics (ICM), volume 1, pages 193–198. IEEE, 2019.\n",
      "[12] Sait Murat Giray. Anatomy of unmanned aerial vehicle hijacking with signal spoofing. In\n",
      "2013 6th International Conference on Recent Advances in Space Technologies (RAST), pages\n",
      "795–800. IEEE, 2013.\n",
      "[13] Bora Ly and Romny Ly. Cybersecurity in unmanned aerial vehicles (uavs). Journal of Cyber\n",
      "Security Technology, 5(2):120–137, 2021.\n",
      "[14] Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell.\n",
      "Adversarial policies: Attacking deep reinforcement learning. arXiv preprint arXiv:1905.10615,\n",
      "2019.\n",
      "[15] Jieyu Lin, Kristina Dzeparoska, Sai Qian Zhang, Alberto Leon-Garcia, and Nicolas Papernot.\n",
      "On the robustness of cooperative multi-agent reinforcement learning. In 2020 IEEE Security\n",
      "and Privacy Workshops (SPW), pages 62–68. IEEE, 2020.\n",
      "[16] Le Cong Dinh, David Henry Mguni, Long Tran-Thanh, Jun Wang, and Yaodong Yang. Online\n",
      "markov decision processes with non-oblivious strategic adversary. Autonomous Agents and\n",
      "Multi-Agent Systems, 37(1):15, 2023.\n",
      "[17] Jean-Michel Lasry and Pierre-Louis Lions. Mean field games. Japanese journal of mathematics,\n",
      "2(1):229–260, 2007.\n",
      "[18] David Kempe, Jon Kleinberg, and Éva Tardos. Maximizing the spread of influence through\n",
      "a social network. In Proceedings of the ninth ACM SIGKDD international conference on\n",
      "Knowledge discovery and data mining, pages 137–146, 2003.\n",
      "[19] Suman Banerjee, Mamata Jenamani, and Dilip Kumar Pratihar. A survey on influence maxi-\n",
      "mization in a social network. Knowledge and Information Systems, 62:3417–3455, 2020.\n",
      "[20] Yandi Li, Haobo Gao, Yunxuan Gao, Jianxiong Guo, and Weili Wu. A survey on influence\n",
      "maximization: From an ml-based combinatorial optimization. ACM Transactions on Knowledge\n",
      "Discovery from Data, 17(9):1–50, 2023.\n",
      "11\n",
      "\n",
      "[21] Nhan H Pham, Lam M Nguyen, Jie Chen, Hoang Thanh Lam, Subhro Das, and Tsui-Wei\n",
      "Weng. Evaluating robustness of cooperative marl: A model-based approach. arXiv preprint\n",
      "arXiv:2202.03558, 2022.\n",
      "[22] Lixia Zan, Xiangbin Zhu, and Zhao-Long Hu. Adversarial attacks on cooperative multi-agent\n",
      "deep reinforcement learning: a dynamic group-based adversarial example transferability method.\n",
      "Complex & Intelligent Systems, 9(6):7439–7450, 2023.\n",
      "[23] Ziyuan Zhou and Guanjun Liu. Robustness testing for multi-agent reinforcement learning: State\n",
      "perturbations on critical agents. arXiv preprint arXiv:2306.06136, 2023.\n",
      "[24] Reuven Cohen and Liran Katzir. The generalized maximum coverage problem. Information\n",
      "Processing Letters, 108(1):15–22, 2008.\n",
      "[25] Ralph Tyrell Rockafellar. Convex analysis, 1970.\n",
      "[26] Ofir Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality. arXiv preprint\n",
      "arXiv:2001.01866, 2020.\n",
      "[27] Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game\n",
      "theoretical perspective. arXiv preprint arXiv:2011.00583, 2020.\n",
      "[28] Minyi Huang, Roland P Malhamé, and Peter E Caines. Large population stochastic dynamic\n",
      "games: closed-loop mckean-vlasov systems and the nash certainty equivalence principle. COM-\n",
      "MUNICATIONS IN INFORMATION AND SYSTEMS, 2006.\n",
      "[29] Xin Guo, Anran Hu, Renyuan Xu, and Junzi Zhang. Learning mean-field games. Advances in\n",
      "neural information processing systems, 32, 2019.\n",
      "[30] Julien Perolat, Sarah Perrin, Romuald Elie, Mathieu Laurière, Georgios Piliouras, Matthieu\n",
      "Geist, Karl Tuyls, and Olivier Pietquin. Scaling up mean field games with online mirror descent.\n",
      "arXiv preprint arXiv:2103.00623, 2021.\n",
      "[31] Paul Muller, Romuald Elie, Mark Rowland, Mathieu Lauriere, Julien Perolat, Sarah Perrin,\n",
      "Matthieu Geist, Georgios Piliouras, Olivier Pietquin, and Karl Tuyls. Learning correlated\n",
      "equilibria in mean-field games. arXiv preprint arXiv:2208.10138, 2022.\n",
      "[32] René Carmona, Mathieu Laurière, and Zongjun Tan. Model-free mean-field reinforcement\n",
      "learning: mean-field mdp and mean-field q-learning. The Annals of Applied Probability,\n",
      "33(6B):5334–5381, 2023.\n",
      "[33] Haotian Gu, Xin Guo, Xiaoli Wei, and Renyuan Xu. Mean-field controls with q-learning for\n",
      "cooperative marl: convergence and complexity analysis. SIAM Journal on Mathematics of Data\n",
      "Science, 3(4):1168–1196, 2021.\n",
      "[34] Washim Uddin Mondal, Mridul Agarwal, Vaneet Aggarwal, and Satish V Ukkusuri. On the\n",
      "approximation of cooperative heterogeneous multi-agent reinforcement learning (marl) using\n",
      "mean field control (mfc). Journal of Machine Learning Research, 23(129):1–46, 2022.\n",
      "[35] Andrea Angiuli, Jean-Pierre Fouque, and Mathieu Laurière. Unified reinforcement q-learning\n",
      "for mean field game and control problems. Mathematics of Control, Signals, and Systems,\n",
      "34(2):217–271, 2022.\n",
      "[36] Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary mean-\n",
      "field games. In Proceedings of the 18th International Conference on Autonomous Agents and\n",
      "MultiAgent Systems, pages 251–259, 2019.\n",
      "[37] Sriram Ganapathi Subramanian, Pascal Poupart, Matthew E Taylor, and Nidhi Hegde. Multi\n",
      "type mean field reinforcement learning. arXiv preprint arXiv:2002.02513, 2020.\n",
      "[38] Sriram Ganapathi Subramanian, Matthew E Taylor, Mark Crowley, and Pascal Poupart. Partially\n",
      "observable mean field reinforcement learning. arXiv preprint arXiv:2012.15791, 2020.\n",
      "12\n",
      "\n",
      "[39] Pier Giuseppe Sessa, Maryam Kamgarpour, and Andreas Krause.\n",
      "Efficient model-based\n",
      "multi-agent reinforcement learning via optimistic equilibrium computation. In International\n",
      "Conference on Machine Learning, pages 19580–19597. PMLR, 2022.\n",
      "[40] Kai Cui, Sascha Hauck, Christian Fabian, and Heinz Koeppl. Learning decentralized partially\n",
      "observable mean field control for artificial collective behavior. arXiv preprint arXiv:2307.06175,\n",
      "2023.\n",
      "[41] Kai Cui, Christian Fabian, Anam Tahir, and Heinz Koeppl. Major-minor mean field multi-agent\n",
      "reinforcement learning. In Forty-first International Conference on Machine Learning, 2024.\n",
      "[42] Jun Guo, Yonghong Chen, Yihang Hao, Zixin Yin, Yin Yu, and Simin Li. Towards comprehen-\n",
      "sive testing on the robustness of cooperative multi-agent reinforcement learning. In Proceedings\n",
      "of the IEEE/CVF conference on computer vision and pattern recognition, pages 115–122, 2022.\n",
      "[43] Simin Li, Jun Guo, Jingqiao Xiu, Pu Feng, Xin Yu, Aishan Liu, Wenjun Wu, and Xianglong Liu.\n",
      "Attacking cooperative multi-agent reinforcement learning by adversarial minority influence.\n",
      "arXiv preprint arXiv:2302.03322, 2023.\n",
      "[44] Kaiqing Zhang, Tao Sun, Yunzhe Tao, Sahika Genc, Sunil Mallya, and Tamer Basar. Robust\n",
      "multi-agent reinforcement learning with model uncertainty. Advances in neural information\n",
      "processing systems, 33:10571–10583, 2020.\n",
      "[45] Laixi Shi, Eric Mazumdar, Yuejie Chi, and Adam Wierman. Sample-efficient robust multi-\n",
      "agent reinforcement learning in the face of enviro\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "path = \"data/MultiAgent.pdf\"\n",
    "test = parsing_paper(path)\n",
    "print(test[:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9df7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION_TITLES = [\n",
    "    \"Abstract\", \"Introduction\", \"Methods\", \"Methodology\", \n",
    "    \"Results\", \"Discussion\", \"Conclusion\", \"References\"\n",
    "]\n",
    "\n",
    "def tldr(pdf_path):\n",
    "    full_text = parsing_paper(path)\n",
    "    parsed_paper = {} # empty dict\n",
    "    found_sections = [] # empty list\n",
    "\n",
    "    for title in SECTION_TITLES:\n",
    "        start_index = full_text.lower().find(title.lower())\n",
    "        if start_index != -1: # if we found the titles\n",
    "            found_sections.append((start_index,title)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
